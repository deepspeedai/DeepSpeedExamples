# DeepSpeed Visual Guide

A visual guide to understanding DeepSpeed's architecture, memory optimization, and communication patterns through diagrams.

---

## Table of Contents

1. [ZeRO Optimization Stages](#zero-optimization-stages)
2. [Memory Layout Comparison](#memory-layout-comparison)
3. [Communication Patterns](#communication-patterns)
4. [Offloading Strategies](#offloading-strategies)
5. [Pipeline Parallelism](#pipeline-parallelism)
6. [Tensor Parallelism](#tensor-parallelism)
7. [3D Parallelism](#3d-parallelism)
8. [Training Timeline](#training-timeline)

---

## ZeRO Optimization Stages

### ZeRO-0: Standard Data Parallel (No Optimization)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 0 (80 GB)                            â”‚
â”‚                                                             â”‚
â”‚  Model Parameters      : 14 GB                              â”‚
â”‚  Gradients            : 14 GB                              â”‚
â”‚  Optimizer States (m,v): 28 GB                              â”‚
â”‚  Activations          : 20 GB                              â”‚
â”‚                         â”€â”€â”€â”€â”€                               â”‚
â”‚  Total                : 76 GB âœ“ Fits!                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 1 (80 GB)                            â”‚
â”‚                                                             â”‚
â”‚  Model Parameters      : 14 GB  (replicated)                â”‚
â”‚  Gradients            : 14 GB  (replicated)                â”‚
â”‚  Optimizer States (m,v): 28 GB  (replicated)                â”‚
â”‚  Activations          : 20 GB                              â”‚
â”‚                         â”€â”€â”€â”€â”€                               â”‚
â”‚  Total                : 76 GB âœ“ Fits!                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problem: Every GPU has full copy of model + optimizer!
Solution: Partition states across GPUs (ZeRO-1/2/3)
```

---

### ZeRO-1: Optimizer State Partitioning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 0 (80 GB)                            â”‚
â”‚                                                             â”‚
â”‚  Model Parameters      : 14 GB  (full copy)                 â”‚
â”‚  Gradients            : 14 GB  (full copy)                 â”‚
â”‚  Optimizer States (m,v): 14 GB  â† Only Partition 0         â”‚
â”‚  Activations          : 20 GB                              â”‚
â”‚                         â”€â”€â”€â”€â”€                               â”‚
â”‚  Total                : 62 GB  (14 GB saved!)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 1 (80 GB)                            â”‚
â”‚                                                             â”‚
â”‚  Model Parameters      : 14 GB  (full copy)                 â”‚
â”‚  Gradients            : 14 GB  (full copy)                 â”‚
â”‚  Optimizer States (m,v): 14 GB  â† Only Partition 1         â”‚
â”‚  Activations          : 20 GB                              â”‚
â”‚                         â”€â”€â”€â”€â”€                               â”‚
â”‚  Total                : 62 GB  (14 GB saved!)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Memory Savings: 4Ã— for optimizer states (per GPU)
Communication: All-Gather parameters after optimizer step
```

---

### ZeRO-2: + Gradient Partitioning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 0 (80 GB)                            â”‚
â”‚                                                             â”‚
â”‚  Model Parameters      : 14 GB  (full copy)                 â”‚
â”‚  Gradients            :  7 GB  â† Only Partition 0          â”‚
â”‚  Optimizer States (m,v): 14 GB  â† Only Partition 0         â”‚
â”‚  Activations          : 20 GB                              â”‚
â”‚                         â”€â”€â”€â”€â”€                               â”‚
â”‚  Total                : 55 GB  (21 GB saved!)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 1 (80 GB)                            â”‚
â”‚                                                             â”‚
â”‚  Model Parameters      : 14 GB  (full copy)                 â”‚
â”‚  Gradients            :  7 GB  â† Only Partition 1          â”‚
â”‚  Optimizer States (m,v): 14 GB  â† Only Partition 1         â”‚
â”‚  Activations          : 20 GB                              â”‚
â”‚                         â”€â”€â”€â”€â”€                               â”‚
â”‚  Total                : 55 GB  (21 GB saved!)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Memory Savings: 8Ã— for (optimizer + gradients) per GPU
Communication: Reduce-Scatter gradients during backward pass
```

---

### ZeRO-3: + Parameter Partitioning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 0 (80 GB)                            â”‚
â”‚                                                             â”‚
â”‚  Model Parameters      :  7 GB  â† Only Partition 0         â”‚
â”‚  Gradients            :  7 GB  â† Only Partition 0          â”‚
â”‚  Optimizer States (m,v): 14 GB  â† Only Partition 0         â”‚
â”‚  Activations          : 20 GB                              â”‚
â”‚                         â”€â”€â”€â”€â”€                               â”‚
â”‚  Total                : 48 GB  (28 GB saved!)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 1 (80 GB)                            â”‚
â”‚                                                             â”‚
â”‚  Model Parameters      :  7 GB  â† Only Partition 1         â”‚
â”‚  Gradients            :  7 GB  â† Only Partition 1          â”‚
â”‚  Optimizer States (m,v): 14 GB  â† Only Partition 1         â”‚
â”‚  Activations          : 20 GB                              â”‚
â”‚                         â”€â”€â”€â”€â”€                               â”‚
â”‚  Total                : 48 GB  (28 GB saved!)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Memory Savings: Linear scaling! Nx GPUs = 1/N memory per GPU
Communication: All-Gather params before forward/backward
```

---

## Memory Layout Comparison

### 7B Model on 2 GPUs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ZeRO-0                                   â”‚
â”‚                                                                  â”‚
â”‚  GPU 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (76 GB)            â”‚
â”‚  GPU 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (76 GB)            â”‚
â”‚                                                                  â”‚
â”‚  Total Memory: 152 GB                                            â”‚
â”‚  Memory per GPU: 76 GB                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ZeRO-1                                   â”‚
â”‚                                                                  â”‚
â”‚  GPU 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (62 GB)                  â”‚
â”‚  GPU 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (62 GB)                  â”‚
â”‚                                                                  â”‚
â”‚  Total Memory: 124 GB                                            â”‚
â”‚  Memory per GPU: 62 GB  (-18% vs ZeRO-0)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ZeRO-2                                   â”‚
â”‚                                                                  â”‚
â”‚  GPU 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (55 GB)                        â”‚
â”‚  GPU 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (55 GB)                        â”‚
â”‚                                                                  â”‚
â”‚  Total Memory: 110 GB                                            â”‚
â”‚  Memory per GPU: 55 GB  (-28% vs ZeRO-0)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ZeRO-3                                   â”‚
â”‚                                                                  â”‚
â”‚  GPU 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (48 GB)                                â”‚
â”‚  GPU 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (48 GB)                                â”‚
â”‚                                                                  â”‚
â”‚  Total Memory: 96 GB                                             â”‚
â”‚  Memory per GPU: 48 GB  (-37% vs ZeRO-0)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend:
  â–ˆ = GPU Memory Used
  â–‘ = GPU Memory Available
```

---

## Communication Patterns

### ZeRO-1: All-Gather After Optimizer Step

```
Step N: Training

GPU 0:  [Paramâ‚€] â”€â”€â”
GPU 1:  [Paramâ‚] â”€â”€â”¤
GPU 2:  [Paramâ‚‚] â”€â”€â”¤
GPU 3:  [Paramâ‚ƒ] â”€â”€â”˜
         â”‚
         â”‚ Compute Forward/Backward
         â”‚ Update Optimizer States
         â–¼

All-Gather (Broadcast updated parameters):

GPU 0:  [Paramâ‚€] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
GPU 1:  [Paramâ‚] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
GPU 2:  [Paramâ‚‚] â”€â”€â”€â”€â”€â”   â”‚   â”‚
GPU 3:  [Paramâ‚ƒ] â”€â”€â”  â”‚   â”‚   â”‚
                   â”‚  â”‚   â”‚   â”‚
                   â–¼  â–¼   â–¼   â–¼
GPU 0:  [Paramâ‚€][Paramâ‚][Paramâ‚‚][Paramâ‚ƒ]  â† Full Parameters
GPU 1:  [Paramâ‚€][Paramâ‚][Paramâ‚‚][Paramâ‚ƒ]  â† Full Parameters
GPU 2:  [Paramâ‚€][Paramâ‚][Paramâ‚‚][Paramâ‚ƒ]  â† Full Parameters
GPU 3:  [Paramâ‚€][Paramâ‚][Paramâ‚‚][Paramâ‚ƒ]  â† Full Parameters
```

---

### ZeRO-2: Reduce-Scatter During Backward

```
Backward Pass: Gradients computed

GPU 0:  [Gradâ‚€][Gradâ‚][Gradâ‚‚][Gradâ‚ƒ]  â† Full Gradients
GPU 1:  [Gradâ‚€][Gradâ‚][Gradâ‚‚][Gradâ‚ƒ]  â† Full Gradients
GPU 2:  [Gradâ‚€][Gradâ‚][Gradâ‚‚][Gradâ‚ƒ]  â† Full Gradients
GPU 3:  [Gradâ‚€][Gradâ‚][Gradâ‚‚][Gradâ‚ƒ]  â† Full Gradients
         â”‚
         â”‚ Reduce-Scatter (Sum + Partition)
         â–¼

GPU 0:  [Gradâ‚€]  â† Sum of Gradâ‚€ from all GPUs
GPU 1:  [Gradâ‚]  â† Sum of Gradâ‚ from all GPUs
GPU 2:  [Gradâ‚‚]  â† Sum of Gradâ‚‚ from all GPUs
GPU 3:  [Gradâ‚ƒ]  â† Sum of Gradâ‚ƒ from all GPUs

Each GPU now updates its partition of optimizer states
```

---

### ZeRO-3: All-Gather Before Forward/Backward

```
Forward Pass for Layer N:

GPU 0:  [Paramâ‚€] â”€â”€â”
GPU 1:  [Paramâ‚] â”€â”€â”¤
GPU 2:  [Paramâ‚‚] â”€â”€â”¤
GPU 3:  [Paramâ‚ƒ] â”€â”€â”˜
         â”‚
         â”‚ All-Gather (Reconstruct full layer)
         â–¼
GPU 0:  [Paramâ‚€][Paramâ‚][Paramâ‚‚][Paramâ‚ƒ]  â† Compute Forward
GPU 1:  [Paramâ‚€][Paramâ‚][Paramâ‚‚][Paramâ‚ƒ]  â† Compute Forward
GPU 2:  [Paramâ‚€][Paramâ‚][Paramâ‚‚][Paramâ‚ƒ]  â† Compute Forward
GPU 3:  [Paramâ‚€][Paramâ‚][Paramâ‚‚][Paramâ‚ƒ]  â† Compute Forward
         â”‚
         â”‚ Forward complete â†’ Release params
         â–¼
GPU 0:  [Paramâ‚€]  â† Keep only own partition
GPU 1:  [Paramâ‚]  â† Keep only own partition
GPU 2:  [Paramâ‚‚]  â† Keep only own partition
GPU 3:  [Paramâ‚ƒ]  â† Keep only own partition

Repeat for backward pass!
```

---

## Offloading Strategies

### CPU Offload: Optimizer States

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 0 (80 GB)                         â”‚
â”‚                                                          â”‚
â”‚  Model Parameters      : 14 GB                           â”‚
â”‚  Gradients            : 14 GB                           â”‚
â”‚  Activations          : 20 GB                           â”‚
â”‚  Optimizer States     :  0 GB  â† Offloaded to CPU       â”‚
â”‚                         â”€â”€â”€â”€â”€                            â”‚
â”‚  Total                : 48 GB  (-37% memory!)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†•
                    PCIe Transfer
                    (~25 GB/s)
                          â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CPU RAM (256 GB)                       â”‚
â”‚                                                          â”‚
â”‚  Optimizer States (m,v): 28 GB                           â”‚
â”‚  DeepSpeedCPUAdam      : Running on CPU                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Trade-off: 10-20% slower, but 37% memory savings on GPU
```

---

### NVMe Offload: Parameters

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 0 (80 GB)                         â”‚
â”‚                                                          â”‚
â”‚  Active Layer Params   :  2 GB  â† Only current layer    â”‚
â”‚  Gradients            : 14 GB                           â”‚
â”‚  Optimizer States     :  0 GB  â† Offloaded to CPU       â”‚
â”‚  Activations          : 20 GB                           â”‚
â”‚                         â”€â”€â”€â”€â”€                            â”‚
â”‚  Total                : 36 GB  (-53% memory!)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†•
                    PCIe Transfer
                          â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CPU RAM (256 GB)                       â”‚
â”‚                                                          â”‚
â”‚  Optimizer States      : 28 GB                           â”‚
â”‚  Parameter Prefetch    :  4 GB                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†•
                    Async I/O
                    (~5 GB/s)
                          â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   NVMe SSD (2 TB)                        â”‚
â”‚                                                          â”‚
â”‚  Model Parameters      : 14 GB  â† All parameters        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Trade-off: 2-5Ã— slower, but can train 100B+ models on single GPU!
```

---

## Pipeline Parallelism

### 4-Stage Pipeline on 4 GPUs

```
Model: 12 Transformer Layers

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 0: Layers 0-2   â”‚ GPU 1: Layers 3-5                     â”‚
â”‚                     â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚ â”‚ Layer 0 â”‚         â”‚ â”‚ Layer 3 â”‚                           â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â”‚      â”‚              â”‚      â”‚                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                           â”‚
â”‚ â”‚ Layer 1 â”‚         â”‚ â”‚ Layer 4 â”‚                           â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â”‚      â”‚              â”‚      â”‚                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                           â”‚
â”‚ â”‚ Layer 2 â”‚         â”‚ â”‚ Layer 5 â”‚                           â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚   P2P Transfer      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 2: Layers 6-8   â”‚ GPU 3: Layers 9-11                    â”‚
â”‚                     â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚ â”‚ Layer 6 â”‚         â”‚ â”‚ Layer 9 â”‚                           â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â”‚      â”‚              â”‚      â”‚                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                           â”‚
â”‚ â”‚ Layer 7 â”‚         â”‚ â”‚ Layer 10â”‚                           â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â”‚      â”‚              â”‚      â”‚                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                           â”‚
â”‚ â”‚ Layer 8 â”‚         â”‚ â”‚ Layer 11â”‚                           â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚   P2P Transfer      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Communication: Point-to-point between adjacent stages
Memory: 3Ã— reduction (each GPU holds 1/4 of model)
```

---

### Pipeline Timeline (Micro-Batches)

```
Time:  â”œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¤

GPU 0: â”‚ Fâ‚  â”‚ Fâ‚‚  â”‚ Fâ‚ƒ  â”‚ Fâ‚„  â”‚ Bâ‚„  â”‚ Bâ‚ƒ  â”‚ Bâ‚‚  â”‚ Bâ‚  â”‚
       â””â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
          â”‚
GPU 1:    â”‚  Fâ‚  â”‚ Fâ‚‚  â”‚ Fâ‚ƒ  â”‚ Fâ‚„  â”‚ Bâ‚„  â”‚ Bâ‚ƒ  â”‚ Bâ‚‚  â”‚ Bâ‚  â”‚
          â””â”€â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
              â”‚
GPU 2:        â”‚  Fâ‚  â”‚ Fâ‚‚  â”‚ Fâ‚ƒ  â”‚ Fâ‚„  â”‚ Bâ‚„  â”‚ Bâ‚ƒ  â”‚ Bâ‚‚  â”‚ Bâ‚  â”‚
              â””â”€â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
                  â”‚
GPU 3:            â”‚  Fâ‚  â”‚ Fâ‚‚  â”‚ Fâ‚ƒ  â”‚ Fâ‚„  â”‚ Bâ‚„  â”‚ Bâ‚ƒ  â”‚ Bâ‚‚  â”‚ Bâ‚  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Legend:
  Fâ‚ = Forward pass for micro-batch 1
  Bâ‚ = Backward pass for micro-batch 1

Observation: Pipeline fills up gradually, then runs efficiently
Efficiency: 4 micro-batches â†’ ~75% pipeline utilization
```

---

## Tensor Parallelism

### Column-wise Split

```
Original Matrix Multiplication: Y = XW

X: (batch, seq_len, hidden)  = (32, 128, 768)
W: (hidden, ff_dim)           = (768, 3072)
Y: (batch, seq_len, ff_dim)   = (32, 128, 3072)

With 2-way Tensor Parallel:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 0                          â”‚
â”‚                                                   â”‚
â”‚  X: (32, 128, 768)  â”€â”€â”                           â”‚
â”‚                       â”‚                           â”‚
â”‚  Wâ‚€: (768, 1536) â”€â”€â”€â”€â”¤  Yâ‚€ = XWâ‚€                 â”‚
â”‚                       â”‚  (32, 128, 1536)          â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 1                          â”‚
â”‚                                                   â”‚
â”‚  X: (32, 128, 768)  â”€â”€â”                           â”‚
â”‚                       â”‚                           â”‚
â”‚  Wâ‚: (768, 1536) â”€â”€â”€â”€â”¤  Yâ‚ = XWâ‚                 â”‚
â”‚                       â”‚  (32, 128, 1536)          â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ Concatenate: Y = [Yâ‚€ | Yâ‚]
                         â–¼
                    (32, 128, 3072)

Communication: All-Reduce after computation
Memory Savings: W split across GPUs (2Ã— reduction)
```

---

## 3D Parallelism

### Combining Data + Tensor + Pipeline Parallel

```
8 Nodes Ã— 8 GPUs = 64 GPUs Total

3D Configuration:
- Data Parallel (DP): 4 way
- Tensor Parallel (TP): 4 way
- Pipeline Parallel (PP): 4 way

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Parallel Group 0                â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Tensor Parallel Group 0                â”‚  â”‚
â”‚  â”‚                                                  â”‚  â”‚
â”‚  â”‚  GPU0 â”€ GPU1 â”€ GPU2 â”€ GPU3                      â”‚  â”‚
â”‚  â”‚   â”‚      â”‚      â”‚      â”‚                         â”‚  â”‚
â”‚  â”‚   â”œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¤ Tensor Parallel        â”‚  â”‚
â”‚  â”‚   â”‚                    â”‚                         â”‚  â”‚
â”‚  â”‚  Stage0  Stage1  Stage2  Stage3 â† Pipeline      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Tensor Parallel Group 1                â”‚  â”‚
â”‚  â”‚  GPU4 â”€ GPU5 â”€ GPU6 â”€ GPU7                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              ...  (2 more TP groups)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data Parallel Groups 1, 2, 3 (similar structure)
...

Communication:
- Within TP group: High bandwidth (NVLink/NVSwitch)
- Between PP stages: P2P transfers
- Across DP groups: All-Reduce gradients

Memory: Model split across TPÃ—PP (16Ã— reduction)
Scale: Can train models 100Ã— larger than single GPU
```

---

## Training Timeline

### Single Training Step with ZeRO-3

```
Time:  0ms     50ms    100ms   150ms   200ms   250ms   300ms
       â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤

GPU 0: â”‚A-Gâ”‚Forwardâ”‚A-Gâ”‚Backwardâ”‚R-Sâ”‚Updateâ”‚Syncâ”‚
       â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜

GPU 1: â”‚A-Gâ”‚Forwardâ”‚A-Gâ”‚Backwardâ”‚R-Sâ”‚Updateâ”‚Syncâ”‚
       â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜

Legend:
  A-G = All-Gather parameters
  R-S = Reduce-Scatter gradients
  Sync = Synchronize between GPUs

Breakdown:
  All-Gather    : 40ms  (13%)  â† ZeRO-3 overhead
  Forward       : 80ms  (27%)
  Backward      : 100ms (33%)
  Reduce-Scatter: 30ms  (10%)  â† ZeRO-2/3 overhead
  Update        : 40ms  (13%)
  Sync          : 10ms  (3%)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total         : 300ms (100%)

Communication Overhead: 80ms (27% of total)
```

---

### Overlapped Communication (Optimized)

```
Time:  0ms     50ms    100ms   150ms   200ms   250ms
       â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤

GPU 0: â”‚A-Gâ”‚Forwardâ”‚A-Gâ”‚Backwardâ”‚Updateâ”‚
       â””â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
              â”‚            â”‚
           Overlap     Overlap
              â”‚            â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Communication in background â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

With overlap_comm=true:
  Computation  : 220ms (88%)
  Communication:  30ms (12%)  â† Hidden by overlap!
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total        : 250ms

Speedup: 20% faster than without overlap!
```

---

## Summary

### ZeRO Stage Selection Flowchart

```
                    Start
                      â”‚
                      â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Model fits in GPU?     â”‚
         â”‚ (with batch size)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚
        Yes               No
         â”‚                 â”‚
         â–¼                 â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ZeRO-0  â”‚      â”‚ Multiple  â”‚
   â”‚ or      â”‚      â”‚ GPUs?     â”‚
   â”‚ ZeRO-1  â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                  â”‚             â”‚
                 Yes           No
                  â”‚             â”‚
                  â–¼             â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Try      â”‚   â”‚ Enable  â”‚
           â”‚ ZeRO-2   â”‚   â”‚ CPU/NVMeâ”‚
           â”‚ or ZeRO-3â”‚   â”‚ Offload â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                â”‚
      Still OOM?         Fits?
          â”‚                â”‚
          â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Enable   â”‚     â”‚ Success!â”‚
    â”‚ Offload  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Additional Resources

- **[ZeRO Paper](https://arxiv.org/abs/1910.02054)** - Original research
- **[Pipeline Parallelism](https://www.deepspeed.ai/tutorials/pipeline/)** - Tutorial
- **[Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed)** - 3D parallelism
- **[ZeRO-3 Concept to Code](./ZeRO3_Concept_to_Code.md)** - Detailed guide

---

**Happy visualizing!** ğŸ¨
