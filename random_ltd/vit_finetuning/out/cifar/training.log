[2022-11-22 10:02:00,070] [INFO] [runner.py:417:main] Using IP address of 192.168.200.181 for node master
[2022-11-22 10:02:00,070] [INFO] [runner.py:508:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJtYXN0ZXIiOiBbMF19 --master_addr=192.168.200.181 --master_port=60000 main_cifar.py --deepspeed_config config/ds_config.json --deepspeed --random_ltd --dataset cifar10vit224 --seed 1234 --printfreq 400 --arch vits16r224 --optimizer sgd --lr 0.0001 --seq_len 197 --scheduler constant --epochs 14 --batchsize 128 --data_outdir check/cifar/
[2022-11-22 10:02:01,786] [INFO] [launch.py:135:main] 0 NCCL_VERSION=2.9.8
[2022-11-22 10:02:01,787] [INFO] [launch.py:142:main] WORLD INFO DICT: {'master': [0]}
[2022-11-22 10:02:01,787] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=1, node_rank=0
[2022-11-22 10:02:01,787] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'master': [0]})
[2022-11-22 10:02:01,787] [INFO] [launch.py:162:main] dist_world_size=1
[2022-11-22 10:02:01,787] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/random_ltd/build.ninja...
Building extension module random_ltd...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module random_ltd...
Time to load random_ltd op: 0.15318989753723145 seconds
Files already downloaded and verified
cifar10
Files already downloaded and verified
cifar10
=> creating model 'vits16r224'
[2022-11-22 10:02:19,811] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.7.6+7fe3dbf3, git-hash=7fe3dbf3, git-branch=staging_data_efficiency_v1
[2022-11-22 10:02:19,812] [INFO] [comm.py:633:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2022-11-22 10:02:19,886] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-11-22 10:02:19,887] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-11-22 10:02:19,887] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-11-22 10:02:19,891] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = SGD
[2022-11-22 10:02:19,891] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = SGD
[2022-11-22 10:02:19,891] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-11-22 10:02:19,891] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.StepLR object at 0x7f8a6fc1e310>
[2022-11-22 10:02:19,891] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:02:19,892] [INFO] [config.py:995:print] DeepSpeedEngine configuration:
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   amp_enabled .................. False
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   amp_params ................... False
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "/vc_data/users/xwu/Token-Dropping/random-ltd-version1/DeepSpeedExamples-internal/random_ltd/vit_finetuning/autotuning_results", 
    "exps_dir": "/vc_data/users/xwu/Token-Dropping/random-ltd-version1/DeepSpeedExamples-internal/random_ltd/vit_finetuning/autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   bfloat16_enabled ............. False
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   checkpoint_parallel_write_pipeline  False
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   checkpoint_tag_validation_enabled  True
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   checkpoint_tag_validation_fail  False
[2022-11-22 10:02:19,892] [INFO] [config.py:999:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8a686946d0>
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   communication_data_type ...... None
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   curriculum_enabled_legacy .... False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   curriculum_params_legacy ..... False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   data_efficiency_config ....... {'enabled': True, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': True, 'random_ltd': {'enabled': True, 'layer_token_lr_schedule': {'enabled': False}, 'total_layer_num': 12, 'random_ltd_layer_num': 10, 'random_ltd_layer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'model_mask_name': None, 'model_type': 'decoder', 'hidden_state_order': 'batch_seq_dim', 'random_ltd_schedule': {'min_value': 32, 'max_value': 197, 'schedule_type': 'fixed_linear', 'schedule_config': {'require_steps': 3910, 'seq_per_step': 8}}, 'global_batch_size': 32, 'micro_batch_size': 32}}}
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   data_efficiency_enabled ...... True
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   dataloader_drop_last ......... False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   disable_allgather ............ False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   dump_state ................... False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   dynamic_loss_scale_args ...... None
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   eigenvalue_enabled ........... False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   eigenvalue_gas_boundary_resolution  1
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   eigenvalue_layer_num ......... 0
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   eigenvalue_max_iter .......... 100
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   eigenvalue_stability ......... 1e-06
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   eigenvalue_tol ............... 0.01
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   eigenvalue_verbose ........... False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   elasticity_enabled ........... False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   fp16_auto_cast ............... None
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   fp16_enabled ................. False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   fp16_master_weights_and_gradients  False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   global_rank .................. 0
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   gradient_accumulation_steps .. 1
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   gradient_clipping ............ 1.0
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   gradient_predivide_factor .... 1.0
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   initial_dynamic_scale ........ 4294967296
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   load_universal_checkpoint .... False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   loss_scale ................... 0
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   memory_breakdown ............. False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f8a68694910>
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   optimizer_legacy_fusion ...... False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   optimizer_name ............... adam
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   pld_enabled .................. False
[2022-11-22 10:02:19,893] [INFO] [config.py:999:print]   pld_params ................... False
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   prescale_gradients ........... True
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   scheduler_name ............... None
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   scheduler_params ............. None
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   sparse_attention ............. None
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   sparse_gradients_enabled ..... False
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   steps_per_print .............. 200
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   train_batch_size ............. 32
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   train_micro_batch_size_per_gpu  32
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   use_node_local_storage ....... False
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   wall_clock_breakdown ......... False
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   world_size ................... 1
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   zero_allow_untested_optimizer  False
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   zero_enabled ................. False
[2022-11-22 10:02:19,894] [INFO] [config.py:999:print]   zero_optimization_stage ...... 0
[2022-11-22 10:02:19,894] [INFO] [config.py:984:print_user_config]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": 200, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.8, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 3e-07
        }
    }, 
    "zero_optimization": {
        "stage": 0
    }, 
    "fp16": {
        "enabled": false
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "wall_clock_breakdown": false, 
    "data_efficiency": {
        "enabled": true, 
        "data_routing": {
            "enabled": true, 
            "random_ltd": {
                "enabled": true, 
                "total_layer_num": 12, 
                "random_ltd_layer_num": 10, 
                "random_ltd_layer_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 
                "model_mask_name": null, 
                "model_type": "decoder", 
                "hidden_state_order": "batch_seq_dim", 
                "random_ltd_schedule": {
                    "min_value": 32, 
                    "max_value": 197, 
                    "schedule_type": "fixed_linear", 
                    "schedule_config": {
                        "require_steps": 3.910000e+03, 
                        "seq_per_step": 8
                    }
                }
            }
        }, 
        "data_sampling": {
            "curriculum_learning": {
            }
        }
    }
}
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.8609664440155029 seconds
Epoch: [0][  0/391]	Time  4.268 ( 4.268)	Loss 3.4360e+00 (3.4360e+00)	Acc@1   7.81 (  7.81)	Acc@5  53.12 ( 53.12)
[2022-11-22 10:03:37,041] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:03:37,042] [INFO] [timer.py:198:stop] 0/200, RunningAvgSamplesPerSec=90.31663802675355, CurrSamplesPerSec=82.74966845933933, MemAllocated=0.72GB, MaxMemAllocated=7.0GB
val[ 0/40]	Time  2.986 ( 2.986)	Loss 3.6274e+00 (3.6274e+00)	Acc@1   8.98 (  8.98)	Acc@5  49.22 ( 49.22)
0 epoch at time 185.19143652915955s | reserved_length 48
iter 391 | LR [0.0001]| val_acc 10.969999313354492 | layer_token 9476288
Epoch: [1][  0/391]	Time  1.505 ( 1.505)	Loss 2.2442e+00 (2.2442e+00)	Acc@1  14.06 ( 14.06)	Acc@5  61.72 ( 61.72)
[2022-11-22 10:05:30,951] [INFO] [logging.py:68:log_dist] [Rank 0] step=400, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:05:30,952] [INFO] [timer.py:198:stop] 0/400, RunningAvgSamplesPerSec=85.55873280400775, CurrSamplesPerSec=74.4479834305781, MemAllocated=0.72GB, MaxMemAllocated=7.52GB
[2022-11-22 10:06:59,989] [INFO] [logging.py:68:log_dist] [Rank 0] step=600, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:06:59,990] [INFO] [timer.py:198:stop] 0/600, RunningAvgSamplesPerSec=80.94502100126826, CurrSamplesPerSec=67.97262614675995, MemAllocated=0.72GB, MaxMemAllocated=8.03GB
val[ 0/40]	Time  2.051 ( 2.051)	Loss 3.5924e+00 (3.5924e+00)	Acc@1  10.55 ( 10.55)	Acc@5  47.27 ( 47.27)
1 epoch at time 213.16975092887878s | reserved_length 64
iter 782 | LR [0.0001]| val_acc 11.920000076293945 | layer_token 21015936
Epoch: [2][  0/391]	Time  1.544 ( 1.544)	Loss 2.0863e+00 (2.0863e+00)	Acc@1  25.00 ( 25.00)	Acc@5  71.09 ( 71.09)
[2022-11-22 10:09:09,305] [INFO] [logging.py:68:log_dist] [Rank 0] step=800, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:09:09,306] [INFO] [timer.py:198:stop] 0/800, RunningAvgSamplesPerSec=76.94541157286308, CurrSamplesPerSec=63.893518520722424, MemAllocated=0.72GB, MaxMemAllocated=8.55GB
[2022-11-22 10:10:53,150] [INFO] [logging.py:68:log_dist] [Rank 0] step=1000, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:10:53,151] [INFO] [timer.py:198:stop] 0/1000, RunningAvgSamplesPerSec=73.54501012954583, CurrSamplesPerSec=59.176048417426145, MemAllocated=0.72GB, MaxMemAllocated=9.09GB
val[ 0/40]	Time  2.029 ( 2.029)	Loss 3.5508e+00 (3.5508e+00)	Acc@1   9.77 (  9.77)	Acc@5  51.56 ( 51.56)
2 epoch at time 241.11620593070984s | reserved_length 80
iter 1173 | LR [0.0001]| val_acc 12.519999504089355 | layer_token 34618944
Epoch: [3][  0/391]	Time  1.612 ( 1.612)	Loss 2.0441e+00 (2.0441e+00)	Acc@1  21.09 ( 21.09)	Acc@5  80.47 ( 80.47)
[2022-11-22 10:13:16,878] [INFO] [logging.py:68:log_dist] [Rank 0] step=1200, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:13:16,879] [INFO] [timer.py:198:stop] 0/1200, RunningAvgSamplesPerSec=70.45535337435912, CurrSamplesPerSec=56.14690977605278, MemAllocated=0.72GB, MaxMemAllocated=9.62GB
[2022-11-22 10:15:15,710] [INFO] [logging.py:68:log_dist] [Rank 0] step=1400, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:15:15,711] [INFO] [timer.py:198:stop] 0/1400, RunningAvgSamplesPerSec=67.63171133558585, CurrSamplesPerSec=51.93513222362125, MemAllocated=0.72GB, MaxMemAllocated=10.17GB
val[ 0/40]	Time  2.124 ( 2.124)	Loss 3.5127e+00 (3.5127e+00)	Acc@1  10.55 ( 10.55)	Acc@5  53.91 ( 53.91)
3 epoch at time 270.82587814331055s | reserved_length 96
iter 1564 | LR [0.0001]| val_acc 13.210000038146973 | layer_token 50282752
Epoch: [4][  0/391]	Time  1.637 ( 1.637)	Loss 2.0212e+00 (2.0212e+00)	Acc@1  24.22 ( 24.22)	Acc@5  78.91 ( 78.91)
[2022-11-22 10:17:55,858] [INFO] [logging.py:68:log_dist] [Rank 0] step=1600, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:17:55,858] [INFO] [timer.py:198:stop] 0/1600, RunningAvgSamplesPerSec=64.91657996957399, CurrSamplesPerSec=48.94036518108643, MemAllocated=0.72GB, MaxMemAllocated=10.72GB
[2022-11-22 10:20:10,389] [INFO] [logging.py:68:log_dist] [Rank 0] step=1800, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:20:10,389] [INFO] [timer.py:198:stop] 0/1800, RunningAvgSamplesPerSec=62.48671136083536, CurrSamplesPerSec=47.3198796217166, MemAllocated=0.72GB, MaxMemAllocated=11.28GB
val[ 0/40]	Time  2.029 ( 2.029)	Loss 3.3991e+00 (3.3991e+00)	Acc@1  12.89 ( 12.89)	Acc@5  57.03 ( 57.03)
4 epoch at time 299.80421257019043s | reserved_length 112
iter 1955 | LR [0.0001]| val_acc 14.9399995803833 | layer_token 68009920
Epoch: [5][  0/391]	Time  1.721 ( 1.721)	Loss 1.8469e+00 (1.8469e+00)	Acc@1  33.59 ( 33.59)	Acc@5  84.38 ( 84.38)
[2022-11-22 10:23:05,069] [INFO] [logging.py:68:log_dist] [Rank 0] step=2000, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:23:05,069] [INFO] [timer.py:198:stop] 0/2000, RunningAvgSamplesPerSec=60.22394920001411, CurrSamplesPerSec=43.90118370349485, MemAllocated=0.72GB, MaxMemAllocated=11.85GB
[2022-11-22 10:25:36,977] [INFO] [logging.py:68:log_dist] [Rank 0] step=2200, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:25:36,977] [INFO] [timer.py:198:stop] 0/2200, RunningAvgSamplesPerSec=58.0302711211572, CurrSamplesPerSec=41.69454053839119, MemAllocated=0.72GB, MaxMemAllocated=12.42GB
val[ 0/40]	Time  2.085 ( 2.085)	Loss 3.1699e+00 (3.1699e+00)	Acc@1  15.62 ( 15.62)	Acc@5  64.06 ( 64.06)
5 epoch at time 331.90209555625916s | reserved_length 128
iter 2346 | LR [0.0001]| val_acc 17.3799991607666 | layer_token 87800448
Epoch: [6][  0/391]	Time  1.752 ( 1.752)	Loss 1.8112e+00 (1.8112e+00)	Acc@1  39.06 ( 39.06)	Acc@5  82.81 ( 82.81)
[2022-11-22 10:28:46,739] [INFO] [logging.py:68:log_dist] [Rank 0] step=2400, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:28:46,740] [INFO] [timer.py:198:stop] 0/2400, RunningAvgSamplesPerSec=56.094513245054095, CurrSamplesPerSec=40.58047448091367, MemAllocated=0.72GB, MaxMemAllocated=13.01GB
[2022-11-22 10:31:35,571] [INFO] [logging.py:68:log_dist] [Rank 0] step=2600, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:31:35,572] [INFO] [timer.py:198:stop] 0/2600, RunningAvgSamplesPerSec=54.14851211437061, CurrSamplesPerSec=37.19453052125877, MemAllocated=0.72GB, MaxMemAllocated=13.6GB
val[ 0/40]	Time  2.104 ( 2.104)	Loss 2.7670e+00 (2.7670e+00)	Acc@1  19.14 ( 19.14)	Acc@5  74.22 ( 74.22)
6 epoch at time 365.16202425956726s | reserved_length 144
iter 2737 | LR [0.0001]| val_acc 21.799999237060547 | layer_token 109651776
Epoch: [7][  0/391]	Time  1.914 ( 1.914)	Loss 1.5624e+00 (1.5624e+00)	Acc@1  36.72 ( 36.72)	Acc@5  96.09 ( 96.09)
[2022-11-22 10:35:05,442] [INFO] [logging.py:68:log_dist] [Rank 0] step=2800, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:35:05,443] [INFO] [timer.py:198:stop] 0/2800, RunningAvgSamplesPerSec=52.32241990113161, CurrSamplesPerSec=35.964618919178186, MemAllocated=0.72GB, MaxMemAllocated=14.19GB
[2022-11-22 10:38:07,794] [INFO] [logging.py:68:log_dist] [Rank 0] step=3000, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:38:07,795] [INFO] [timer.py:198:stop] 0/3000, RunningAvgSamplesPerSec=50.70310735297397, CurrSamplesPerSec=35.352774615530095, MemAllocated=0.72GB, MaxMemAllocated=14.8GB
val[ 0/40]	Time  2.179 ( 2.179)	Loss 2.0598e+00 (2.0598e+00)	Acc@1  35.16 ( 35.16)	Acc@5  83.59 ( 83.59)
7 epoch at time 390.96979331970215s | reserved_length 160
iter 3128 | LR [0.0001]| val_acc 33.2599983215332 | layer_token 133566464
Epoch: [8][  0/391]	Time  2.013 ( 2.013)	Loss 1.6446e+00 (1.6446e+00)	Acc@1  42.19 ( 42.19)	Acc@5  86.72 ( 86.72)
[2022-11-22 10:41:48,100] [INFO] [logging.py:68:log_dist] [Rank 0] step=3200, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:41:48,101] [INFO] [timer.py:198:stop] 0/3200, RunningAvgSamplesPerSec=49.237104426155284, CurrSamplesPerSec=34.13426217805304, MemAllocated=0.72GB, MaxMemAllocated=15.41GB
[2022-11-22 10:45:09,381] [INFO] [logging.py:68:log_dist] [Rank 0] step=3400, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:45:09,382] [INFO] [timer.py:198:stop] 0/3400, RunningAvgSamplesPerSec=47.72817142090195, CurrSamplesPerSec=31.80557087961357, MemAllocated=0.72GB, MaxMemAllocated=16.03GB
val[ 0/40]	Time  2.216 ( 2.216)	Loss 1.0670e+00 (1.0670e+00)	Acc@1  66.80 ( 66.80)	Acc@5  96.09 ( 96.09)
8 epoch at time 424.1727337837219s | reserved_length 176
iter 3519 | LR [0.0001]| val_acc 63.689998626708984 | layer_token 159544512
Epoch: [9][  0/391]	Time  2.096 ( 2.096)	Loss 8.8429e-01 (8.8429e-01)	Acc@1  66.41 ( 66.41)	Acc@5  98.44 ( 98.44)
[2022-11-22 10:49:07,673] [INFO] [logging.py:68:log_dist] [Rank 0] step=3600, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:49:07,674] [INFO] [timer.py:198:stop] 0/3600, RunningAvgSamplesPerSec=46.38046720293777, CurrSamplesPerSec=31.30438254780285, MemAllocated=0.72GB, MaxMemAllocated=16.65GB
[2022-11-22 10:52:39,461] [INFO] [logging.py:68:log_dist] [Rank 0] step=3800, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:52:39,462] [INFO] [timer.py:198:stop] 0/3800, RunningAvgSamplesPerSec=45.13502570171775, CurrSamplesPerSec=29.14035960359582, MemAllocated=0.72GB, MaxMemAllocated=17.93GB
val[ 0/40]	Time  2.054 ( 2.054)	Loss 3.5701e-01 (3.5701e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
9 epoch at time 448.95574712753296s | reserved_length 192
iter 3910 | LR [0.0001]| val_acc 90.90999603271484 | layer_token 187585920
Epoch: [10][  0/391]	Time  2.058 ( 2.058)	Loss 3.2708e-01 (3.2708e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
[2022-11-22 10:56:53,186] [INFO] [logging.py:68:log_dist] [Rank 0] step=4000, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 10:56:53,187] [INFO] [timer.py:198:stop] 0/4000, RunningAvgSamplesPerSec=43.924403068564054, CurrSamplesPerSec=28.096997618978836, MemAllocated=0.72GB, MaxMemAllocated=17.93GB
[2022-11-22 11:00:42,498] [INFO] [logging.py:68:log_dist] [Rank 0] step=4200, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 11:00:42,499] [INFO] [timer.py:198:stop] 0/4200, RunningAvgSamplesPerSec=42.77566538448087, CurrSamplesPerSec=28.048574047963413, MemAllocated=0.72GB, MaxMemAllocated=17.93GB
val[ 0/40]	Time  2.156 ( 2.156)	Loss 1.5025e-01 (1.5025e-01)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
10 epoch at time 477.3163368701935s | reserved_length 197
iter 4301 | LR [0.0001]| val_acc 96.0999984741211 | layer_token 217049088
Epoch: [11][  0/391]	Time  2.070 ( 2.070)	Loss 7.7886e-02 (7.7886e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
[2022-11-22 11:05:03,825] [INFO] [logging.py:68:log_dist] [Rank 0] step=4400, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 11:05:03,826] [INFO] [timer.py:198:stop] 0/4400, RunningAvgSamplesPerSec=41.78839027909469, CurrSamplesPerSec=28.109796352366512, MemAllocated=0.72GB, MaxMemAllocated=17.93GB
[2022-11-22 11:08:53,081] [INFO] [logging.py:68:log_dist] [Rank 0] step=4600, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 11:08:53,081] [INFO] [timer.py:198:stop] 0/4600, RunningAvgSamplesPerSec=40.92120350947847, CurrSamplesPerSec=28.093645395343852, MemAllocated=0.72GB, MaxMemAllocated=17.93GB
val[ 0/40]	Time  2.184 ( 2.184)	Loss 1.0392e-01 (1.0392e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
11 epoch at time 480.4685637950897s | reserved_length 197
iter 4692 | LR [0.0001]| val_acc 97.22000122070312 | layer_token 246627456
Epoch: [12][  0/391]	Time  2.175 ( 2.175)	Loss 1.4460e-01 (1.4460e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
[2022-11-22 11:13:14,678] [INFO] [logging.py:68:log_dist] [Rank 0] step=4800, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 11:13:14,679] [INFO] [timer.py:198:stop] 0/4800, RunningAvgSamplesPerSec=40.16016411440734, CurrSamplesPerSec=28.033061275895566, MemAllocated=0.72GB, MaxMemAllocated=17.93GB
[2022-11-22 11:17:04,090] [INFO] [logging.py:68:log_dist] [Rank 0] step=5000, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 11:17:04,091] [INFO] [timer.py:198:stop] 0/5000, RunningAvgSamplesPerSec=39.48047398366816, CurrSamplesPerSec=28.152112073395575, MemAllocated=0.72GB, MaxMemAllocated=17.93GB
val[ 0/40]	Time  2.093 ( 2.093)	Loss 8.0803e-02 (8.0803e-02)	Acc@1  97.27 ( 97.27)	Acc@5 100.00 (100.00)
12 epoch at time 480.5284731388092s | reserved_length 197
iter 5083 | LR [0.0001]| val_acc 97.69999694824219 | layer_token 276205824
Epoch: [13][  0/391]	Time  2.094 ( 2.094)	Loss 5.6260e-02 (5.6260e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
[2022-11-22 11:21:25,587] [INFO] [logging.py:68:log_dist] [Rank 0] step=5200, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 11:21:25,588] [INFO] [timer.py:198:stop] 0/5200, RunningAvgSamplesPerSec=38.876190250499064, CurrSamplesPerSec=28.26498609786438, MemAllocated=0.72GB, MaxMemAllocated=17.93GB
[2022-11-22 11:25:15,036] [INFO] [logging.py:68:log_dist] [Rank 0] step=5400, skipped=0, lr=[0.0001], mom=[0.9]
[2022-11-22 11:25:15,037] [INFO] [timer.py:198:stop] 0/5400, RunningAvgSamplesPerSec=38.32984362432623, CurrSamplesPerSec=28.04859749414077, MemAllocated=0.72GB, MaxMemAllocated=17.93GB
val[ 0/40]	Time  2.171 ( 2.171)	Loss 7.1221e-02 (7.1221e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
13 epoch at time 480.6546013355255s | reserved_length 197
iter 5474 | LR [0.0001]| val_acc 97.97000122070312 | layer_token 305784192
[2022-11-22 11:27:14,442] [INFO] [launch.py:350:main] Process 29248 exits successfully.
